{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP3ErtTqV8JXowBC1w0pOPw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ee0122ng/EBAC_Semester2_AirBnbDataAnalysis/blob/main/LSTM_BinaryClassifications.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-eRlb85zDgV",
        "outputId": "8f1acb96-c9f3-4d7b-9e30-973d8c9fbdd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# import google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import datasets\n",
        "import pandas as pd\n",
        "\n",
        "df_hm6A = pd.read_csv('/content/drive/My Drive/EBAC_Capstone/data_binary/hm6A_with_ROS.csv', usecols=lambda x: x!='Unnamed: 0')\n",
        "# df_hm6Am = pd.read_csv('/content/drive/My Drive/EBAC_Capstone/data_binary/hm6Am_with_ROS.csv', usecols=lambda x: x!='Unnamed: 0')\n",
        "# df_hPsi = pd.read_csv('/content/drive/My Drive/EBAC_Capstone/data_binary/hPsi_with_ROS.csv', usecols=lambda x: x!='Unnamed: 0')\n",
        "# df_hm7G = pd.read_csv('/content/drive/My Drive/EBAC_Capstone/data_binary/hm7G_with_ROS.csv', usecols=lambda x: x!='Unnamed: 0')\n",
        "# df_hm5U = pd.read_csv('/content/drive/My Drive/EBAC_Capstone/data_binary/hm5U_with_ROS.csv', usecols=lambda x: x!='Unnamed: 0')\n",
        "# df_hTm = pd.read_csv('/content/drive/My Drive/EBAC_Capstone/data_binary/hTm_with_ROS.csv', usecols=lambda x: x!='Unnamed: 0')\n",
        "# df_hm5C = pd.read_csv('/content/drive/My Drive/EBAC_Capstone/data_binary/hm5C_with_ROS.csv', usecols=lambda x: x!='Unnamed: 0')\n",
        "# df_Atol = pd.read_csv('/content/drive/My Drive/EBAC_Capstone/data_binary/Atol_with_ROS.csv', usecols=lambda x: x!='Unnamed: 0')\n",
        "# df_hGm = pd.read_csv('/content/drive/My Drive/EBAC_Capstone/data_binary/hGm_with_ROS.csv', usecols=lambda x: x!='Unnamed: 0')\n",
        "# df_hAm = pd.read_csv('/content/drive/My Drive/EBAC_Capstone/data_binary/hAm_with_ROS.csv', usecols=lambda x: x!='Unnamed: 0')\n",
        "# df_hm1A = pd.read_csv('/content/drive/My Drive/EBAC_Capstone/data_binary/hm1A_with_ROS.csv', usecols=lambda x: x!='Unnamed: 0')\n",
        "# df_hCm = pd.read_csv('/content/drive/My Drive/EBAC_Capstone/data_binary/hCm_with_ROS.csv', usecols=lambda x: x!='Unnamed: 0')"
      ],
      "metadata": {
        "id": "Cwo8dIyv1eXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hm6A.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRrYO6h8Pp1N",
        "outputId": "f52ced6b-e10e-4e32-e8a6-c5fe59e69f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(309214, 102)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2vec Model Training"
      ],
      "metadata": {
        "id": "DaIsj24HKI1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define function to extract kmer from the original dataset\n",
        "from itertools import product\n",
        "\n",
        "nucleotide = ['A', 'C', 'G', 'T', 'N']\n",
        "\n",
        "def get_ngrams(seq, k):\n",
        "  for i in range(len(seq) - k + 1):\n",
        "    yield seq[i:i+k]\n",
        "\n",
        "def extract_ngrams(data, k):\n",
        "  combinations = sorted([''.join(str(s) for s in t) for t in product(nucleotide, repeat=k)])\n",
        "  ngrams = []\n",
        "  ngrams_map = {}\n",
        "\n",
        "  # get list of nucleotide combinations and populate to count\n",
        "  for g in get_ngrams(data, k):\n",
        "    ngrams.append(g)\n",
        "    if g in ngrams_map:\n",
        "      ngrams_map[g] += 1\n",
        "    else:\n",
        "      ngrams_map[g] = 1\n",
        "\n",
        "  return combinations, ngrams, ngrams_map\n",
        "\n",
        "# extract k-mer\n",
        "def extract_kmer(k, data):\n",
        "\n",
        "  # extract 2grams from the processed dataset\n",
        "  ngrams = [*map(extract_ngrams, data.sum(axis=1).tolist(), [k]*data.shape[0])]\n",
        "\n",
        "  # transform 2grams features into dataframe\n",
        "  features = [*map(lambda x: x[1], ngrams)]\n",
        "  column = [*map(lambda x: 'p_'+str(x), range(len(features[0])))]\n",
        "  df = pd.DataFrame(features, columns=column)\n",
        "\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "jiaAdvUULKYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# import original dataset\n",
        "df_raw = pd.read_csv('/content/drive/My Drive/EBAC_Capstone/data/train_in.csv', usecols=lambda x : x != 'Unnamed: 0')\n",
        "df = df_raw.iloc[:, 450:550]\n",
        "\n",
        "# extract k-mer\n",
        "df_kmer = extract_kmer(2, df)\n",
        "\n",
        "# hold values in a list as corpus\n",
        "corpus_RNA = df_kmer.values.tolist()"
      ],
      "metadata": {
        "id": "afMhuxR-KMvN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "09d34e3c-1ec8-450c-f411-2dc4ade9baef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-294f5d62cf50>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# import original dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/EBAC_Capstone/data/train_in.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m450\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m550\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1776\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-294f5d62cf50>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# import original dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/EBAC_Capstone/data/train_in.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m450\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m550\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_kmer.to_csv('/content/drive/My Drive/EBAC_Capstone/data/train_in_2mer_withNoise.csv', index=False)"
      ],
      "metadata": {
        "id": "U4vvjC2WMG16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train Word2Vec model with the corpus\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# train the model\n",
        "# vector_size = number of features to produce by the NN\n",
        "# window = window for skip-grams (forward and backward from the target word)\n",
        "# min_count = minimum number of word used in the model\n",
        "# worker = number of parallel works\n",
        "%time model = Word2Vec(sentences=corpus_RNA, vector_size=100, window=5, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "EPWZ0PMzKxCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the Word2Vec model with noise\n",
        "model.save('/content/drive/My Drive/EBAC_Capstone/model/word2vec_model_withNoise.bin')"
      ],
      "metadata": {
        "id": "2EpG3VdqMnKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load trained word2vec model\n",
        "import gensim\n",
        "\n",
        "word2vec = gensim.models.word2vec.Word2Vec.load(\"/content/drive/My Drive/EBAC_Capstone/model/word2vec_model_withNoise.bin\")"
      ],
      "metadata": {
        "id": "CO2otgg3-33I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get 2mer nucleotide combinations\n",
        "from itertools import product\n",
        "\n",
        "nucleotide = ['A', 'C', 'G', 'T', 'N']\n",
        "combinations = sorted([''.join(str(s) for s in t) for t in product(nucleotide, repeat=2)])"
      ],
      "metadata": {
        "id": "9epsPEvSAPgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for c in combinations:\n",
        "  try:\n",
        "    if word2vec.wv[c].shape is not None:\n",
        "      print('%s: value' %c)\n",
        "    else:\n",
        "      print('%s: null' %c)\n",
        "    print('-'*30)\n",
        "  except:\n",
        "    print('%s: null' %c)\n",
        "    print('-'*30)"
      ],
      "metadata": {
        "id": "gBJOFDsOAYyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### binary classification for multi classes"
      ],
      "metadata": {
        "id": "s4nNBSVSzLn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train LSTM model from features embedded via Word2Vec on 2mer dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from itertools import product\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "import os\n",
        "from torch.cuda.amp import GradScaler, autocast # Automatic Mixed Precision (AMP)\n",
        "\n",
        "# enable GPU\n",
        "gpu = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# define a class to build LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "    super(LSTMModel, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "    self.activation = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.float()\n",
        "    x = x.to(gpu)\n",
        "    # h0 and c0 are used to initialize the lstm model\n",
        "    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(gpu)\n",
        "    c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(gpu)\n",
        "    out, _ = self.lstm(x, (h0, c0))\n",
        "    out = self.fc(out[:, -1, :])\n",
        "    # out = self.fc(out[:, -1])\n",
        "    return out\n",
        "\n",
        "# define a class to build trainloader\n",
        "class MyDataset(Dataset):\n",
        "  def __init__(self, features, targets):\n",
        "    self.features = features\n",
        "    self.targets = targets\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.features)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.features[idx], self.targets[idx]\n",
        "\n",
        "'''\n",
        "sub-function to extract k-mer dataset from the original dataset\n",
        "'''\n",
        "nucleotide = ['A', 'C', 'G', 'T', 'N']\n",
        "\n",
        "def get_ngrams(seq, k):\n",
        "  for i in range(len(seq) - k + 1):\n",
        "    yield seq[i:i+k]\n",
        "\n",
        "def extract_ngrams(data, k):\n",
        "  combinations = sorted([''.join(str(s) for s in t) for t in product(nucleotide, repeat=k)])\n",
        "  ngrams = []\n",
        "  ngrams_map = {}\n",
        "\n",
        "  # get list of nucleotide combinations and populate to count\n",
        "  for g in get_ngrams(data, k):\n",
        "    ngrams.append(g)\n",
        "    if g in ngrams_map:\n",
        "      ngrams_map[g] += 1\n",
        "    else:\n",
        "      ngrams_map[g] = 1\n",
        "\n",
        "  return combinations, ngrams, ngrams_map\n",
        "\n",
        "'''\n",
        "sub-function for train_test_split\n",
        "'''\n",
        "def convert_to_word2vec_features(data, k):\n",
        "  print('embedding features...')\n",
        "  nucleotide = ['A', 'C', 'G', 'T', 'N']\n",
        "\n",
        "  # load word2vec model\n",
        "  model = Word2Vec.load('/content/drive/My Drive/EBAC_Capstone/model/word2vec_model_withNoise.bin')\n",
        "\n",
        "  # encode the 2mer dataset with new features generated through gensim.models.word2vec\n",
        "  mapping_word2vec = {}\n",
        "  combinations = sorted([''.join(str(s) for s in t) for t in product(nucleotide, repeat=k)])\n",
        "  for c in combinations:\n",
        "    # combination might not seen by word2vec embedding\n",
        "    try:\n",
        "      mapping_word2vec[c] = model.wv[c]\n",
        "    except:\n",
        "      continue\n",
        "\n",
        "  cols = data.columns\n",
        "  df_wv_encoded = pd.DataFrame()\n",
        "  for c in cols:\n",
        "    df_wv_encoded[c] = data[c].map(mapping_word2vec)\n",
        "\n",
        "  return df_wv_encoded\n",
        "\n",
        "def convert_dataframe_to_tensors(X_train, X_valid, y_train, y_valid):\n",
        "  print('coverting to tensors...')\n",
        "  # convert dataframe to numpy array\n",
        "  arr_train = np.array(X_train.values.tolist(), dtype=np.float16)\n",
        "  arr_valid = np.array(X_valid.values.tolist(), dtype=np.float16)\n",
        "  arr_train_y = np.array(y_train.ravel().tolist(), dtype=np.int16)\n",
        "  arr_valid_y = np.array(y_valid.ravel().tolist(), dtype=np.int16)\n",
        "\n",
        "  # convert the datasets to Pytorch tensors\n",
        "  X_train = torch.tensor(arr_train, dtype=torch.float16)\n",
        "  X_valid = torch.tensor(arr_valid, dtype=torch.float16)\n",
        "  y_train = torch.tensor(arr_train_y, dtype=torch.int16)\n",
        "  y_valid = torch.tensor(arr_valid_y, dtype=torch.int16)\n",
        "\n",
        "  # # convert the datasets to Pytorch tensors\n",
        "  # concatenated_Xtrain = None\n",
        "  # concatenated_Xvalid = None\n",
        "  # concatenated_ytrain = None\n",
        "  # concatenated_yvalid = None\n",
        "\n",
        "  # batchSize = 250\n",
        "  # for i in range(0, arr_train.shape[0], batchSize):\n",
        "  #   batch_Xtrain = arr_train[i: i+batchSize]\n",
        "  #   batch_Xvalid = arr_valid[i: i+batchSize]\n",
        "  #   batch_ytrain = arr_train_y[i: i+batchSize]\n",
        "  #   batch_yvalid = arr_valid_y[i: i+batchSize]\n",
        "\n",
        "  #   if concatenated_Xtrain is None:\n",
        "  #     concatenated_Xtrain = torch.tensor(batch_Xtrain, dtype=torch.float32)\n",
        "  #     concatenated_Xvalid = torch.tensor(batch_Xvalid, dtype=torch.float32)\n",
        "  #     concatenated_ytrain = torch.tensor(batch_ytrain, dtype=torch.float32)\n",
        "  #     concatenated_yvalid = torch.tensor(batch_yvalid, dtype=torch.float32)\n",
        "  #   else:\n",
        "  #     concatenated_Xtrain = torch.cat((concatenated_Xtrain, torch.tensor(batch_Xtrain, dtype=torch.float32)), dim=0)\n",
        "  #     concatenated_Xvalid = torch.cat((concatenated_Xvalid, torch.tensor(batch_Xvalid, dtype=torch.float32)), dim=0)\n",
        "  #     concatenated_ytrain = torch.cat((concatenated_ytrain, torch.tensor(batch_ytrain, dtype=torch.float32)), dim=0)\n",
        "  #     concatenated_yvalid = torch.cat((concatenated_yvalid, torch.tensor(batch_yvalid, dtype=torch.float32)), dim=0)\n",
        "\n",
        "  return X_train, X_valid, y_train, y_valid\n",
        "  # return concatenated_Xtrain, concatenated_Xvalid, concatenated_ytrain, concatenated_yvalid\n",
        "\n",
        "def split_dataset(data):\n",
        "  print('splitting dataset...')\n",
        "  # transform features with word2vec\n",
        "  # get word2vec embedding for k=2\n",
        "  df_X = convert_to_word2vec_features(data.iloc[:, :-1], 2) # have dependent column insert to the last\n",
        "  df_y = data.iloc[:, -1]\n",
        "\n",
        "  # split the dataset into train and validation sets\n",
        "  X_train, X_valid, y_train, y_valid = sklearn.model_selection.train_test_split(df_X, df_y, random_state=123, test_size=0.3)\n",
        "\n",
        "  # convert dataframe to tensors\n",
        "  X_train, X_valid, y_train, y_valid = convert_dataframe_to_tensors(X_train, X_valid, y_train, y_valid)\n",
        "\n",
        "  return X_train, X_valid, y_train, y_valid\n",
        "\n",
        "def prepare_dataloaders(X_train, X_valid, y_train, y_valid, batchSize=64):\n",
        "  print('preparing dataloader...')\n",
        "  # get trainloader\n",
        "  train_dataset = MyDataset(X_train, y_train)\n",
        "  trainloader = DataLoader(train_dataset, shuffle=True, num_workers=4, batch_size=batchSize)\n",
        "\n",
        "  # get validloader\n",
        "  valid_dataset = MyDataset(X_valid, y_valid)\n",
        "  validloader = DataLoader(valid_dataset, shuffle=False, num_workers=4, batch_size=batchSize)\n",
        "\n",
        "  return trainloader, validloader\n",
        "\n",
        "def train_LSTM_model(df, hidden_size, num_layers, output_size, num_epochs, batch_size, datasetName):\n",
        "\n",
        "  os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
        "\n",
        "  # split dataframe into featrues and output\n",
        "  features = df.iloc[:, :-1]\n",
        "  outputs = df.iloc[:, -1].to_frame()\n",
        "\n",
        "  # check if 2mer dataset is ready\n",
        "  df = pd.DataFrame()\n",
        "  try:\n",
        "    df = pd.read_csv('/content/drive/My Drive/EBAC_Capstone/data_binary/encoded/' + 'train_in_' + datasetName + '.csv', usecols=lambda x: x!='Unnamed: 0')\n",
        "    print('dataframe %s available' %datasetName)\n",
        "  except:\n",
        "    # transform features to 2mer\n",
        "    result_2g = [*map(extract_ngrams, features.sum(axis=1).tolist(), [2]*features.shape[0])]\n",
        "    features_2g = [*map(lambda x: x[1], result_2g)]\n",
        "    df_2mer = pd.DataFrame(features_2g)\n",
        "\n",
        "    # convert outputs to machine readable\n",
        "    outputs.loc[outputs['TARGET'] == 'NonMoD', 'TARGET'] = 0\n",
        "    outputs.loc[outputs['TARGET'] != 0, 'TARGET'] = 1\n",
        "\n",
        "    # reform the dataset\n",
        "    df = pd.concat([df_2mer, outputs], axis=1)\n",
        "\n",
        "    # export dataset\n",
        "    df.to_csv('/content/drive/My Drive/EBAC_Capstone/data_binary/encoded/' + 'train_in_' + datasetName + '.csv', index=False)\n",
        "\n",
        "  # enable GPU\n",
        "  gpu = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "  # free up memory\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  # step1: prepare and split the data\n",
        "  X_train, X_valid, y_train, y_valid = split_dataset(df)\n",
        "\n",
        "  # step2: prepare dataloader\n",
        "  trainloader, validloader = prepare_dataloaders(X_train, X_valid, y_train, y_valid, batchSize=batch_size)\n",
        "\n",
        "  # create an instance of LSTM model\n",
        "  # input_size = number of input neurons\n",
        "  # hidden_size = number of hidden neurons\n",
        "  # num_layer = number of LSTM layer\n",
        "  # output_size = number of output neurons\n",
        "  input_size = X_train[0].shape[1]\n",
        "  print('training model...')\n",
        "  model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
        "\n",
        "  # define loss function and optimizer\n",
        "  # criterion = nn.CrossEntropyLoss()\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "  # train a LSTM model\n",
        "  length = len(trainloader)\n",
        "\n",
        "  scaler = GradScaler()\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train() # set model to training mode\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "      inputs, targets = data\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # move input, target and model to gpu\n",
        "      model = model.to(gpu)\n",
        "      inputs = inputs.to(gpu)\n",
        "      targets = targets.to(gpu)\n",
        "\n",
        "      # # Reshape the target tensor to [batch_size, 1]\n",
        "      targets = targets.float()\n",
        "      targets = targets.view(-1, 1)\n",
        "\n",
        "      with autocast():\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "      scaler.scale(loss).backward()\n",
        "      scaler.step(optimizer)\n",
        "      scaler.update()\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      # free up resources\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    # calculate average loss for the epoch\n",
        "    avg_loss = total_loss / (len(trainloader) // batch_size)\n",
        "\n",
        "    # evaluate on the validation set\n",
        "    model.eval() # set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "      model = model.to(gpu)\n",
        "      X_valid = X_valid.to(gpu)\n",
        "\n",
        "      y_valid = y_valid.to(gpu)\n",
        "      y_valid = y_valid.float()\n",
        "      y_valid = y_valid.view(-1, 1)\n",
        "\n",
        "      val_outputs = model(X_valid)\n",
        "      val_outputs = torch.relu(val_outputs) # use the activation function to convert the outputs\n",
        "      val_loss = criterion(val_outputs, y_valid)\n",
        "\n",
        "      # free up resources\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}, Val Loss: {val_loss.item()}\")\n",
        "  return model"
      ],
      "metadata": {
        "id": "LvOlRF6LzK7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate LSTM model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix as cfm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "  # evaluate on the validation set\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    model = model.to(gpu)\n",
        "    X_test = X_test.to(gpu)\n",
        "\n",
        "    y_test = y_test.to(gpu)\n",
        "    y_test = y_test.float()\n",
        "    y_test = y_test.view(-1, 1) # convert shape from [1] to [1,1]\n",
        "\n",
        "    val_outputs = model(X_test)\n",
        "\n",
        "  # Move tensors to CPU and convert to NumPy arrays\n",
        "  val_outputs = val_outputs.cpu()\n",
        "  y_test = y_test.cpu()\n",
        "\n",
        "  # evaluate the accuracy\n",
        "  # predicted = torch.argmax(val_outputs, dim=1)\n",
        "  predicted_relu = torch.relu(val_outputs)\n",
        "  predicted = (predicted_relu > 0.5).float()\n",
        "  accuracy = (predicted == y_test).float().mean()\n",
        "  print('Accuracy: {:.2f}%'.format(accuracy * 100))\n",
        "\n",
        "  # precision tp / (tp + fp)\n",
        "  precision = precision_score(y_test, predicted)\n",
        "  print('Precision: %f' % precision)\n",
        "\n",
        "  # recall: tp / (tp + fn)\n",
        "  recall = recall_score(y_test, predicted)\n",
        "  print('Recall: %f' % recall)\n",
        "\n",
        "  # f1: 2 tp / (2 tp + fp + fn)\n",
        "  f1 = f1_score(y_test, predicted)\n",
        "  print('F1 score: %f' % f1)\n",
        "\n",
        "  # visualize confusion matrix\n",
        "  cnf_matrix = cfm(y_test, predicted)\n",
        "  class_names=[0,1] # name  of classes\n",
        "  fig, ax = plt.subplots()\n",
        "  tick_marks = np.arange(len(class_names))\n",
        "  plt.xticks(tick_marks, class_names)\n",
        "  plt.yticks(tick_marks, class_names)\n",
        "\n",
        "  # create heatmap\n",
        "  sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
        "  ax.xaxis.set_label_position(\"top\")\n",
        "  plt.tight_layout()\n",
        "  plt.title('Confusion matrix', y=1.1)\n",
        "  plt.ylabel('Actual label')\n",
        "  plt.xlabel('Predicted label')"
      ],
      "metadata": {
        "id": "Gv_rGuMe1PI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train model on each subset"
      ],
      "metadata": {
        "id": "4inVnMqC-L-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "********** hm6A **********\n",
        "'''\n",
        "# train LSTM model\n",
        "%time model = train_LSTM_model(df_hm6A, hidden_size=16, num_layers=2, output_size=1, num_epochs=20, batch_size=64, datasetName='hm6A')\n",
        "\n",
        "# save model\n",
        "torch.save(model, '/content/drive/My Drive/EBAC_Capstone/model/03 LSTM_epoch20_hidden100_layer2_batch150/LSTM_hm6A.pth')\n",
        "\n",
        "# evaluate performance for hm6A\n",
        "# get valid dataset\n",
        "X_train, X_valid, y_train, y_valid = split_dataset(df_hm6A)\n",
        "\n",
        "# get accuracy\n",
        "evaluate_model(model, X_valid, y_valid)"
      ],
      "metadata": {
        "id": "C3acXvuH-eIp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d16e8757-120c-48ec-9aef-16d299bcbe80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataframe hm6A available\n",
            "splitting dataset...\n",
            "embedding features...\n",
            "coverting to tensors...\n",
            "preparing dataloader...\n",
            "training model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICxdsgqSf8A0",
        "outputId": "c8b8966d-39d0-4163-d0af-686a95b7518f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jul 12 14:43:09 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P0    31W /  70W |  13655MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "********** hm6Am **********\n",
        "'''\n",
        "# train LSTM model\n",
        "%time model = train_LSTM_model(df_hm6Am, hidden_size=100, num_layers=2, output_size=1, num_epochs=20, batch_size=150, datasetName='hm6Am')\n",
        "\n",
        "# save model\n",
        "torch.save(model, '/content/drive/My Drive/EBAC_Capstone/model/03 LSTM_epoch20_hidden100_layer2_batch150/LSTM_hm6Am.pth')\n",
        "\n",
        "# evaluate performance for hm6Am\n",
        "# get valid dataset\n",
        "X_train, X_valid, y_train, y_valid = split_dataset(df_hm6Am)\n",
        "\n",
        "# get accuracy\n",
        "evaluate_model(model, X_valid, y_valid)"
      ],
      "metadata": {
        "id": "dNnrDm7GMmGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "********** hPsi **********\n",
        "'''\n",
        "# train LSTM model\n",
        "%time model = train_LSTM_model(df_hPsi, hidden_size=100, num_layers=2, output_size=1, num_epochs=20, batch_size=150, datasetName='hPsi')\n",
        "\n",
        "# save model\n",
        "torch.save(model, '/content/drive/My Drive/EBAC_Capstone/model/03 LSTM_epoch20_hidden100_layer2_batch150/LSTM_hPsi.pth')\n",
        "\n",
        "# evaluate performance for hPsi\n",
        "# get valid dataset\n",
        "X_train, X_valid, y_train, y_valid = split_dataset(df_hPsi)\n",
        "\n",
        "# get accuracy\n",
        "evaluate_model(model, X_valid, y_valid)"
      ],
      "metadata": {
        "id": "g6qNh3kmNByH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "********** hm7G **********\n",
        "'''\n",
        "# train LSTM model\n",
        "%time model = train_LSTM_model(df_hm7G, hidden_size=100, num_layers=2, output_size=1, num_epochs=20, batch_size=150, datasetName='hm7G')\n",
        "\n",
        "# save model\n",
        "torch.save(model, '/content/drive/My Drive/EBAC_Capstone/model/03 LSTM_epoch20_hidden100_layer2_batch150/LSTM_hm7G.pth')\n",
        "\n",
        "# evaluate performance for hm7G\n",
        "# get valid dataset\n",
        "X_train, X_valid, y_train, y_valid = split_dataset(df_hm7G)\n",
        "\n",
        "# get accuracy\n",
        "evaluate_model(model, X_valid, y_valid)"
      ],
      "metadata": {
        "id": "NVgrSOjZNRC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "********** hm5U **********\n",
        "'''\n",
        "# train LSTM model\n",
        "%time model = train_LSTM_model(df_hm5U, hidden_size=100, num_layers=2, output_size=1, num_epochs=20, batch_size=150, datasetName='hm5U')\n",
        "\n",
        "# save model\n",
        "torch.save(model, '/content/drive/My Drive/EBAC_Capstone/model/03 LSTM_epoch20_hidden100_layer2_batch150/LSTM_hm5U.pth')\n",
        "\n",
        "# evaluate performance for hm5U\n",
        "# get valid dataset\n",
        "X_train, X_valid, y_train, y_valid = split_dataset(df_hm5U)\n",
        "\n",
        "# get accuracy\n",
        "evaluate_model(model, X_valid, y_valid)"
      ],
      "metadata": {
        "id": "BgHseb0-NaEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "********** hTm **********\n",
        "'''\n",
        "# train LSTM model\n",
        "%time model = train_LSTM_model(df_hTm, hidden_size=100, num_layers=2, output_size=1, num_epochs=20, batch_size=150, datasetName='hTm')\n",
        "\n",
        "# save model\n",
        "torch.save(model, '/content/drive/My Drive/EBAC_Capstone/model/03 LSTM_epoch20_hidden100_layer2_batch150/LSTM_hTm.pth')\n",
        "\n",
        "# evaluate performance for hTm\n",
        "# get valid dataset\n",
        "X_train, X_valid, y_train, y_valid = split_dataset(df_hTm)\n",
        "\n",
        "# get accuracy\n",
        "evaluate_model(model, X_valid, y_valid)"
      ],
      "metadata": {
        "id": "H2nSWLP7NhJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "********** hm5C **********\n",
        "'''\n",
        "# train LSTM model\n",
        "%time model = train_LSTM_model(df_hm5C, hidden_size=100, num_layers=2, output_size=1, num_epochs=20, batch_size=150, datasetName='hm5C')\n",
        "\n",
        "# save model\n",
        "torch.save(model, '/content/drive/My Drive/EBAC_Capstone/model/03 LSTM_epoch20_hidden100_layer2_batch150/LSTM_hm5C.pth')\n",
        "\n",
        "# evaluate performance for hm5C\n",
        "# get valid dataset\n",
        "X_train, X_valid, y_train, y_valid = split_dataset(df_hm5C)\n",
        "\n",
        "# get accuracy\n",
        "evaluate_model(model, X_valid, y_valid)"
      ],
      "metadata": {
        "id": "vYCKUIDeNyMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "********** Atol **********\n",
        "'''\n",
        "# train LSTM model\n",
        "%time model = train_LSTM_model(df_Atol, hidden_size=100, num_layers=2, output_size=1, num_epochs=20, batch_size=150, datasetName='Atol')\n",
        "\n",
        "# save model\n",
        "torch.save(model, '/content/drive/My Drive/EBAC_Capstone/model/03 LSTM_epoch20_hidden100_layer2_batch150/LSTM_Atol.pth')\n",
        "\n",
        "# evaluate performance for Atol\n",
        "# get valid dataset\n",
        "X_train, X_valid, y_train, y_valid = split_dataset(df_Atol)\n",
        "\n",
        "# get accuracy\n",
        "evaluate_model(model, X_valid, y_valid)"
      ],
      "metadata": {
        "id": "abijZ29fN_9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "********** hGm **********\n",
        "'''\n",
        "# train LSTM model\n",
        "%time model = train_LSTM_model(df_hGm, hidden_size=100, num_layers=2, output_size=1, num_epochs=20, batch_size=150, datasetName='hGm')\n",
        "\n",
        "# save model\n",
        "torch.save(model, '/content/drive/My Drive/EBAC_Capstone/model/03 LSTM_epoch20_hidden100_layer2_batch150/LSTM_hGm.pth')\n",
        "\n",
        "# evaluate performance for hGm\n",
        "# get valid dataset\n",
        "X_train, X_valid, y_train, y_valid = split_dataset(df_hGm)\n",
        "\n",
        "# get accuracy\n",
        "evaluate_model(model, X_valid, y_valid)"
      ],
      "metadata": {
        "id": "OM4O3inMOGG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "********** hAm **********\n",
        "'''\n",
        "# train LSTM model\n",
        "%time model = train_LSTM_model(df_hAm, hidden_size=100, num_layers=2, output_size=1, num_epochs=20, batch_size=150, datasetName='hAm')\n",
        "\n",
        "# save model\n",
        "torch.save(model, '/content/drive/My Drive/EBAC_Capstone/model/03 LSTM_epoch20_hidden100_layer2_batch150/LSTM_hAm.pth')\n",
        "\n",
        "# evaluate performance for hAm\n",
        "# get valid dataset\n",
        "X_train, X_valid, y_train, y_valid = split_dataset(df_hAm)\n",
        "\n",
        "# get accuracy\n",
        "evaluate_model(model, X_valid, y_valid)"
      ],
      "metadata": {
        "id": "kuVhUaTdOPh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "********** hm1A **********\n",
        "'''\n",
        "# train LSTM model\n",
        "%time model = train_LSTM_model(df_hm1A, hidden_size=100, num_layers=2, output_size=1, num_epochs=20, batch_size=150, datasetName='hm1A')\n",
        "\n",
        "# save model\n",
        "torch.save(model, '/content/drive/My Drive/EBAC_Capstone/model/03 LSTM_epoch20_hidden100_layer2_batch150/LSTM_hm1A.pth')\n",
        "\n",
        "# evaluate performance for hm1A\n",
        "# get valid dataset\n",
        "X_train, X_valid, y_train, y_valid = split_dataset(df_hm1A)\n",
        "\n",
        "# get accuracy\n",
        "evaluate_model(model, X_valid, y_valid)"
      ],
      "metadata": {
        "id": "-N1nKleLOZep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "********** hCm **********\n",
        "'''\n",
        "# train LSTM model\n",
        "%time model = train_LSTM_model(df_hCm, hidden_size=100, num_layers=2, output_size=1, num_epochs=20, batch_size=150, datasetName='hCm')\n",
        "\n",
        "# save model\n",
        "torch.save(model, '/content/drive/My Drive/EBAC_Capstone/model/03 LSTM_epoch20_hidden100_layer2_batch150/LSTM_hCm.pth')\n",
        "\n",
        "# evaluate performance for hCm\n",
        "# get valid dataset\n",
        "X_train, X_valid, y_train, y_valid = split_dataset(df_hCm)\n",
        "\n",
        "# get accuracy\n",
        "evaluate_model(model, X_valid, y_valid)"
      ],
      "metadata": {
        "id": "dVhDDIsoOh4f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}